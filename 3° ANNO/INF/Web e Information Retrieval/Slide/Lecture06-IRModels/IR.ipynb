{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#initialize the vectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#Add the files in the corpus, a document on each line\n",
    "corpus=[\"Racing games\",\n",
    "        \"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]\n",
    "\n",
    "#creates the model\n",
    "model=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.31622777 0.25       0.28867513]\n",
      " [0.31622777 1.         0.31622777 0.36514837]\n",
      " [0.25       0.31622777 1.         0.4330127 ]\n",
      " [0.28867513 0.36514837 0.4330127  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos=cosine_similarity(model)\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13)\n",
      "['about' 'cars' 'describes' 'document' 'game' 'games' 'general' 'in' 'is'\n",
      " 'nice' 'racing' 'this' 'video']\n",
      "[[0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      " [0 1 1 1 0 0 0 0 0 0 1 1 0]\n",
      " [1 0 0 1 0 1 1 1 1 0 0 1 1]\n",
      " [0 0 0 0 1 0 0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(model.shape)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stop words\n",
    "sw=stopwords.words('english')\n",
    "\n",
    "#initialize the vectorizer\n",
    "vectorizer=CountVectorizer(stop_words=sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the files in the corpus, a document on each line\n",
    "corpus=[\"Racing games\",\n",
    "        \"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]\n",
    "\n",
    "#creates the model\n",
    "model=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "['cars' 'describes' 'document' 'game' 'games' 'general' 'nice' 'racing'\n",
      " 'video']\n",
      "[[0 0 0 0 1 0 0 1 0]\n",
      " [1 1 1 0 0 0 0 1 0]\n",
      " [0 0 1 0 1 1 0 0 1]\n",
      " [0 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(model.shape)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import snowball\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "        sw=stopwords.words('english')\n",
    "        stemmer=snowball.SnowballStemmer(language=\"english\")\n",
    "        tokens=word_tokenize(text)\n",
    "        pruned=[stemmer.stem(t) for t in tokens if re.search(r\"^\\w\",t) and not t in sw]\n",
    "        return pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "['car' 'describ' 'document' 'game' 'general' 'nice' 'race' 'video']\n",
      "[[0 0 0 1 0 0 1 0]\n",
      " [1 1 1 0 0 0 1 0]\n",
      " [0 0 1 1 1 0 0 1]\n",
      " [0 0 0 1 0 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdipenta/webIR2024/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initialize the vectorizer\n",
    "vectorizer=CountVectorizer(tokenizer=my_tokenizer)\n",
    "\n",
    "#Add the files in the corpus, a document on each line\n",
    "corpus=[\"Racing games\",\n",
    "        \"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]\n",
    "\n",
    "#creates the model\n",
    "model=vectorizer.fit_transform(corpus)\n",
    "\n",
    "#prints the model\n",
    "print(model.shape)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.35355339 0.35355339 0.70710678]\n",
      " [0.35355339 1.         0.25       0.25      ]\n",
      " [0.35355339 0.25       1.         0.5       ]\n",
      " [0.70710678 0.25       0.5        1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos=cosine_similarity(model)\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import snowball\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "        sw=stopwords.words('english')\n",
    "        stemmer=snowball.SnowballStemmer(language=\"english\")\n",
    "        tokens=word_tokenize(text)\n",
    "        pruned=[stemmer.stem(t) for t in tokens if re.search(r\"^\\w\",t) and not t in sw]\n",
    "        return pruned\n",
    "\n",
    "#initialize the vectorizer\n",
    "vectorizer=TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "\n",
    "#Add the files in the corpus, a document on each line\n",
    "corpus=[\"Racing games\",\n",
    "        \"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]\n",
    "\n",
    "#creates the model\n",
    "model=vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "['car' 'describ' 'document' 'game' 'general' 'nice' 'race' 'video']\n",
      "[[0.         0.         0.         0.70710678 0.         0.\n",
      "  0.70710678 0.        ]\n",
      " [0.57457953 0.57457953 0.4530051  0.         0.         0.\n",
      "  0.36674667 0.        ]\n",
      " [0.         0.         0.4842629  0.39205255 0.61422608 0.\n",
      "  0.         0.4842629 ]\n",
      " [0.         0.         0.         0.40892206 0.         0.64065543\n",
      "  0.40892206 0.5051001 ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.shape)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.25932906 0.27722302 0.57830313]\n",
      " [0.25932906 1.         0.21937356 0.1499708 ]\n",
      " [0.27722302 0.21937356 1.         0.40492018]\n",
      " [0.57830313 0.1499708  0.40492018 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos=cosine_similarity(model)\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.70710678 0.         0.\n",
      "  0.70710678 0.        ]]\n",
      "[[1.         0.25932906 0.27722302 0.57830313]]\n"
     ]
    }
   ],
   "source": [
    "#adds a query to the model\n",
    "query=vectorizer.transform([\"racing game GT7\"])\n",
    "print(query.toarray())\n",
    "\n",
    "\n",
    "cos=cosine_similarity(query,model)\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import snowball\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    \"\"\"tokenization function\"\"\"\n",
    "    sw=stopwords.words('english')\n",
    "    stemmer=snowball.SnowballStemmer(language=\"english\")\n",
    "    tokens=word_tokenize(text)\n",
    "    pruned=[stemmer.stem(t.lower()) for t in tokens \\\n",
    "            if re.search(r\"^\\w\",t) and not t.lower() in sw]\n",
    "    return pruned\n",
    "\n",
    "documents=[\"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.0000001 , 0.07613309, 0.07613309], dtype=float32), array([0.07613309, 1.        , 0.19339646], dtype=float32), array([0.07613309, 0.19339646, 1.        ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "texts=[my_tokenizer(d) for d in documents]\n",
    "\n",
    "#creates the dictionary for the document corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "#creates a bag of word corpus\n",
    "bow_corpus=[dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#creates a tf-idf model from the bag of word corpus\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "#creates an index that facilitates the computation of similarities\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus],len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[['document', 'describ', 'race', 'car'], ['document', 'video', 'game', 'general'], ['nice', 'race', 'video', 'game']]\n",
      "[array([1.0000001 , 0.07613309, 0.07613309], dtype=float32), array([0.07613309, 1.        , 0.19339646], dtype=float32), array([0.07613309, 0.19339646, 1.        ], dtype=float32)]\n",
      "[1.0000001  0.07613309 0.07613309]\n"
     ]
    }
   ],
   "source": [
    "print(bow_corpus[0])\n",
    "tfidf[bow_corpus[0]]\n",
    "print(texts)\n",
    "print(list(index))\n",
    "print(index[tfidf[bow_corpus[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.17312077), (1, 0.21988432), (2, 0.43976864)]\n"
     ]
    }
   ],
   "source": [
    "#tokenizes the query\n",
    "query_document = my_tokenizer(\"racing games\")\n",
    "#indexes the query using the documents' dictionary\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "#computes the similarity between the query and the documents\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
