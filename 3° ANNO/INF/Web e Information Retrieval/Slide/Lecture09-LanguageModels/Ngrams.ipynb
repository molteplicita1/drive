{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d318d271-c37e-46b7-85fc-82728ef4c155",
   "metadata": {},
   "source": [
    "# Introduction to N-Grams with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b0815-e7c4-4dde-a0cd-e89831104b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple API overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f69a4f-9b3d-4e60-a076-1113bceae46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = [['I','want','to','go','home'], ['This', 'file', 'contains', 'a', 'critical', 'bug']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f293c70-d981-4b02-9fff-03233927d539",
   "metadata": {},
   "source": [
    "Getting all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2867e85-7621-4455-a9b8-8040c50a2bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'want'), ('want', 'to'), ('to', 'go'), ('go', 'home')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "list(bigrams(text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642b2c6-4f9e-4c4f-b723-3e155b9e0bbc",
   "metadata": {},
   "source": [
    "Padding the sentence on both ends to mark its beginning and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a41bee3-8af8-425e-b004-fc1797b9a39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'want', 'to', 'go', 'home', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "paddedSent=list(pad_both_ends(text[0], n=2))\n",
    "print(paddedSent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115630e-135e-45cf-be41-6667e1d9b568",
   "metadata": {},
   "source": [
    "Bigrams again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9dca73b-6aca-4364-98d7-384e93c82333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'I'),\n",
       " ('I', 'want'),\n",
       " ('want', 'to'),\n",
       " ('to', 'go'),\n",
       " ('go', 'home'),\n",
       " ('home', '</s>')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(paddedSent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e362d-aa3c-4e98-ad1d-3898fe92bed0",
   "metadata": {},
   "source": [
    "Everygrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c11a0cc-aa80-4ccd-ad0b-fed255f14a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'I'),\n",
       " ('<s>', 'I', 'want'),\n",
       " ('I',),\n",
       " ('I', 'want'),\n",
       " ('I', 'want', 'to'),\n",
       " ('want',),\n",
       " ('want', 'to'),\n",
       " ('want', 'to', 'go'),\n",
       " ('to',),\n",
       " ('to', 'go'),\n",
       " ('to', 'go', 'home'),\n",
       " ('go',),\n",
       " ('go', 'home'),\n",
       " ('go', 'home', '</s>'),\n",
       " ('home',),\n",
       " ('home', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import everygrams\n",
    "list(everygrams(paddedSent, max_len=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86ebc5-f63a-4f0f-8ad7-d3b19a2430b5",
   "metadata": {},
   "source": [
    "If we want to build a training set from all our sentences we need to flatten them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd99404e-1f48-407a-983a-c4932c8e1b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(2, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2aa7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'want', 'to', 'go', 'home'],\n",
       " ['This', 'file', 'contains', 'a', 'critical', 'bug']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d584857b-8ed0-4294-b71b-b1fb0d284ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<generator object everygrams at 0x10eb647b0>,\n",
       " <generator object everygrams at 0x10eb646a0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f528c0-a357-4503-82a4-81da83b82e60",
   "metadata": {},
   "source": [
    "Importing the Maximum Likelihood Estimator and initializing it. The parameter is the maximum n-gram order the model will handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e1ece7-e411-45b8-a133-93e253e370e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "MyModel = MLE(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "081fb6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.lm.models.MLE at 0x10ac86d80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbc123-4973-4708-9873-da77656f139c",
   "metadata": {},
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47bccfd-e9af-40eb-a906-43a09d5696c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MyModel.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951b7e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.lm.models.MLE at 0x10ac86d80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fed9ef6f-5c04-4a38-8abc-ba5a89ce5d18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 14 items>\n"
     ]
    }
   ],
   "source": [
    "print(MyModel.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48aa810-a65e-4d13-ab5e-1f7bbad6593d",
   "metadata": {},
   "source": [
    "How big is its vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efd986e5-5ddf-4dd7-ba77-0b21abd12c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MyModel.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fcae9b-562b-455d-b7a3-13416d80d055",
   "metadata": {},
   "source": [
    "Checking whether words are in the vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ca5b14-0de8-4459-a741-0cab318c0fba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'critical'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyModel.vocab.lookup(\"critical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143cc08-4d65-458f-9b54-cccf3f3f0060",
   "metadata": {},
   "source": [
    "## Training a model from a non-trivial corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2333259-47da-4cf0-9a09-e3babae437af",
   "metadata": {},
   "source": [
    "Let's use Trump Tweets from here https://github.com/MarkHershey/CompleteTrumpTweetsArchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ccb8040-2c2c-451f-b047-e49bffaffe62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"tweets.csv\") as f:\n",
    "    lines=f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c50e78-842e-4efb-9f60-ec78d9832459",
   "metadata": {},
   "source": [
    "Cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf1ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "054bfd5a-1305-46ce-81f3-ad2f52f90dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\#'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\#'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/mg/35_tvr857b9dfpvqyjd1991c0000gp/T/ipykernel_17277/1968880897.py:7: SyntaxWarning: invalid escape sequence '\\#'\n",
      "  text=re.sub(\"[@\\#]\\S+\",\"\",text)\n",
      "/var/folders/mg/35_tvr857b9dfpvqyjd1991c0000gp/T/ipykernel_17277/1968880897.py:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  text=re.sub(\"https?://\\S+\",\"\",text)\n",
      "/var/folders/mg/35_tvr857b9dfpvqyjd1991c0000gp/T/ipykernel_17277/1968880897.py:10: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  text=re.sub(\"pic\\.twitter\\.com\\S+\",\"\",text)\n",
      "/var/folders/mg/35_tvr857b9dfpvqyjd1991c0000gp/T/ipykernel_17277/1968880897.py:17: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  cleaned=[w.lower() for w in words if re.search(\"\\w+\",w)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "def cleanUp(text):\n",
    "    #remove newlines\n",
    "    text=text.strip()\n",
    "    #remove tags\n",
    "    text=re.sub(\"[@\\#]\\S+\",\"\",text)\n",
    "    #remove URLS\n",
    "    text=re.sub(\"https?://\\S+\",\"\",text)\n",
    "    text=re.sub(\"pic\\.twitter\\.com\\S+\",\"\",text)\n",
    "    #tokenize the tweet into sentences\n",
    "    sentences=sent_tokenize(text)\n",
    "    corpus=[]\n",
    "    for sentence in sentences:\n",
    "        words=word_tokenize(sentence)\n",
    "        #take only words from tweets\n",
    "        cleaned=[w.lower() for w in words if re.search(\"\\w+\",w)]\n",
    "        corpus.append(cleaned)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a11d43-bc0b-4b2d-bab9-7e817f4cba30",
   "metadata": {},
   "source": [
    "Cleaning up all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a2278f6-6cef-4f32-93f1-fdd3ea89d27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allSentences=[]\n",
    "for line in lines:\n",
    "    allSentences.extend(cleanUp(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf1cdf-3bf0-428b-9918-fc7ba1cb7285",
   "metadata": {},
   "source": [
    "creating the training set and vocabulary, up to 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7485389c-80d4-4836-9915-1ba76f919b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(3, allSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba647e86-0244-44b7-b4ea-53ce6fc328b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrumpModel = MLE(3)\n",
    "TrumpModel.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b2de0-5fba-40d8-8059-b85949e7398d",
   "metadata": {},
   "source": [
    "Let's experiment with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af45f736-0684-4650-bf64-b9f4214c04d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'interviewed']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrumpModel.generate(2, [\"i\",\"will\"],random_seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ecbc019-c3db-4010-a9ec-e1e139e91061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'again']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrumpModel.generate(2, [\"make\",\"america\"],random_seed=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f826076c-9a84-45d4-b552-5fa8f36b69ac",
   "metadata": {},
   "source": [
    "Let's evaluate the perplexity for some sequences of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d19a741a-1bca-41aa-9fe6-9f4efa9fbb66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7922304393542845"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrumpModel.perplexity([['make','america'],['great','again']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3e2611b-45ac-4791-a0ec-26d78cf1ac66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382.68996985395387"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrumpModel.perplexity([['make','america'],['healthy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca5151b5-5b22-4969-a72a-975e1ac1369a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015395876571475454"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrumpModel.score(\"america\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4eb63e35-adde-4f51-8bc8-49eed4a096d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrumpModel.counts[['great']]['again']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142782e-07db-4b68-bddc-b5bbddfe709a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
